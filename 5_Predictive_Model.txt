import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_squared_error

# -----------------------------
# Load Data
# -----------------------------
def load_data(path):
    data = pd.read_csv(path)
    X = data[['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research']]
    y = data['Chance of Admit']
    return X, y

# -----------------------------
# Build ML Pipeline
# -----------------------------
def build_pipeline(model):
    return Pipeline([
        ('scaler', StandardScaler()),
        ('model', model)
    ])

# -----------------------------
# Evaluate Model
# -----------------------------
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    return r2, rmse

# -----------------------------
# Main Function
# -----------------------------
def main():
    # Step 1: Load dataset
    X, y = load_data("Synthetic_Graduate_Admissions.csv")

    # Step 2: Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Step 3: Define models
    models = {
        "Linear Regression": LinearRegression(),
        "Decision Tree": DecisionTreeRegressor(random_state=42),
        "Random Forest": RandomForestRegressor(random_state=42),
        "Support Vector Regressor": SVR(kernel='rbf'),
        "KNN Regressor": KNeighborsRegressor(n_neighbors=5)
    }

    # Step 4: Train & Evaluate
    results = []
    for name, model in models.items():
        pipe = build_pipeline(model)
        pipe.fit(X_train, y_train)
        r2, rmse = evaluate_model(pipe, X_test, y_test)
        results.append({"Model": name, "R¬≤ Score": round(r2, 3), "RMSE": round(rmse, 3)})

    # Step 5: Display results
    results_df = pd.DataFrame(results)
    print("\n================ Model Performance Summary ================")
    print(results_df.to_string(index=False))

    # Step 6: Predict for a new sample
    sample = pd.DataFrame([[320, 110, 4, 4.5, 4.0, 9.0, 1]],
                          columns=['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research'])
    best_model = build_pipeline(RandomForestRegressor(random_state=42))
    best_model.fit(X, y)
    pred = best_model.predict(sample)
    print(f"\nüéØ Predicted Chance of Admission: {pred[0]*100:.2f}%")

    # Step 7: Optional ‚Äî Save results to CSV
    results_df.to_csv("model_results.csv", index=False)
    print("\n‚úÖ Results saved to 'model_results.csv'")

# -----------------------------
# Run Program
# -----------------------------
if __name__ == "__main__":
    main()














#
THEORY
AIM:
To develop and evaluate multiple machine learning regression models using Python for predicting the chance of admission of students based on academic and profile features such as GRE, TOEFL, CGPA, and research experience.

OBJECTIVE:

1. To apply various regression algorithms for prediction tasks.
2. To understand the workflow of data preprocessing, model training, and evaluation using pipelines.
3. To compare the performance of different models using R¬≤ and RMSE metrics.
4. To automate the prediction process and save model performance results for analysis.

THEORY:

1. Introduction to Regression Models:
   Regression models are used to predict continuous numerical outcomes. In this program, the goal is to predict the probability (or chance) of a student‚Äôs admission based on input parameters like GRE, TOEFL scores, CGPA, and others. Different algorithms like Linear Regression, Decision Tree, Random Forest, Support Vector Regressor (SVR), and K-Nearest Neighbors (KNN) are used to compare performance.

2. Data Loading and Preparation:
   The dataset `Synthetic_Graduate_Admissions.csv` contains features such as GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, and Research, with the target variable ‚ÄúChance of Admit.‚Äù Data is divided into training and testing sets using `train_test_split` to evaluate model generalization.

3. Machine Learning Pipeline:
   A pipeline is created using `Pipeline()` from scikit-learn, which combines preprocessing (StandardScaler for normalization) and model fitting in one step. It ensures consistent data handling and efficient workflow.

4. Model Training:
   Each regression model (Linear Regression, Decision Tree, Random Forest, SVR, and KNN) is trained using the training data. The Random Forest and Decision Tree are ensemble and tree-based models, SVR uses kernel-based learning, and KNN predicts outcomes based on neighboring data points.

5. Model Evaluation:
   Model performance is evaluated using two metrics:

* R¬≤ Score (Coefficient of Determination): Measures how well the model fits the data; higher values indicate better fit.
* RMSE (Root Mean Squared Error): Measures the average prediction error; lower values indicate higher accuracy.

6. Model Comparison and Prediction:
   The results of all models are compared, and the best-performing one (typically Random Forest) is used for final prediction. The code also predicts the chance of admission for a new student profile based on input scores and features.

7. Output and Storage:
   All model performances are saved in a CSV file named `model_results.csv`, providing a record for further reference or visualization.

CONCLUSION:
This program demonstrates the end-to-end machine learning pipeline for regression analysis. It compares multiple models on prediction accuracy and error metrics, identifies the most effective model, and applies it to real-time prediction. The use of pipelines ensures modular, clean, and reproducible code, showing how ML can be efficiently used for academic admission prediction and similar predictive analytics applications.

